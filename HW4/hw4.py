# -*- coding: utf-8 -*-
"""HW4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12FeMoSFjsubWOTbh2RkzOj25n7QPIm84

# **HW4**
# Christopher Walen, COP 4630, Fall 2019

As discussed yesterday in class, summarize and describe the different concepts/methods/algorithms that you have learned in this course.

Use a Colab notebook. Make sure that you organize the material logically by using sections/subsections. Also, use code cell to include code snippets.

I suggest that you group everything into five categories:

0. General concepts (for instance, what is artificial intelligence, machine learning, deep learning)

1. Building a model (for instance, here you can talk about the structure of a convent, what it components are etc.)

2. Comping a model (for instance, you can talk here about optimizers, learning rate etc.)

3. Training a model (for instance, you can talk about overfitting/underfitting)

4.  Finetuning  a pretrained model (describe how you proceed)

# **Foreword**

All code that is present in this pseudo-article comes from my GitHub repository that came as a result of this class, CAP 4630. In this way, it is by no means fully complete or fully 'sound' as far as code can go for AI, and I do not guarantee it to work completely as I am still learning.

The repository can be found here: https://github.com/MrJellimann/machine_learning_homework

I will also say outright that the following is mainly my interpretation of the material we learned in class with Professor Wocjan, with supplementary information from his lecture slides.

# **Category 1 - General Concepts**

The field of Artificial Intelligence is a large and robust field that contains many subsets of AI that all accomplish different tasks or goals.

The first big distinction is the difference between Artificial Intelligence, and Machine Learning. While Machine Learning is a part of AI, it is not the be-all-end-all of AI - in fact it is a subset of AI that focuses on training itself based on data that follows the following structure:

    Input  -> +---+
              |   | -> Rules
    Output -> +---+

After training itself upon this data that has the desired output included, the ML machine generates a set of rules for dealing with the data that it was given. Then we do the following:

    Input  -> +---+
              |   | -> Predictions
    Rules  -> +---+

We then compare those predictions to the actual output (which was previously hidden from the machine) to get our accuracy model for use against unknown data. For example, if we had a bunch of pictures of dogs that we trained the AI on of all the different breeds, if we then showed the AI a new picture of a new dog, it should be able to give a guess with X% confidence that it is of breed Y.

In a more general sense, I take Artificial Intelligence to be:

```the art of taking a computer and teaching it to engage in behaviors that would otherwise be deemed behaviors made by an intelligent, living being```

This is a broad definition, but I would say this applies to things as simple as the glorified state machines that we see in modern Video Games (granted they can be more complicated than simple state machines), all the way up to Personal Assistants i.e. Alexa or Google Assistant, or Face Recognition Software.

In this sense I'd like to co-opt another definition prescribed to AI:

```A computer system able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.```

Since this course focused on Machine Learning, we'll explore the concepts and applications of what I learned over the course as that applies to ML.

# **Category 2 - Building a Model**

In Machine Learning, when working with the data set in question we have what we call the model.

The model is the relationship between the features and labels of a particular data set, where the features are the characteristics of an item in the data set, and the label is the 'answer' so to speak.

The example given in class was that of a data set wherein the leaf length and width of a series of plants was recorded - these being the features - and then stored with the name of the species corresponding to that plant - this being the label.

One of the assignments that we had in class (specifically HW1, Question 3) was to look at the MNIST hand-written numbers dataset and determine the best-fit image for a given number in the data set. To achieve this, we had to create a model that would represent each number and add data that our machine would find as it took in the MNIST data set. In this instance, our model was simply a two-axis graph, where each point on the graph represented the alpha-intensity of a pixel in the number image.

In this case, the features are the alpha values at each pixel in the image, and the labels are the actual number that each image is representing.

We can achieve this simply with the following:
"""

from keras.datasets import mnist
import numpy as np

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

import matplotlib.pyplot as plt

digit_0 = []
digit_1 = []
digit_2 = []
digit_3 = []
digit_4 = []
digit_5 = []
digit_6 = []
digit_7 = []
digit_8 = []
digit_9 = []

"""Each list will hold all of the data that we've found for each number, and store it. In effect we're sorting the data so that we can better train with it further down into the code. Here we actually 'train' the data (though for this question we did not have to make any predictions with the data)."""

for x in range(len(train_labels)):
  if train_labels[x] == 0:
    digit_0.append(train_images[x])
  if train_labels[x] == 1:
    digit_1.append(train_images[x])
  if train_labels[x] == 2:
    digit_2.append(train_images[x])
  if train_labels[x] == 3:
    digit_3.append(train_images[x])
  if train_labels[x] == 4:
    digit_4.append(train_images[x])
  if train_labels[x] == 5:
    digit_5.append(train_images[x])
  if train_labels[x] == 6:
    digit_6.append(train_images[x])
  if train_labels[x] == 7:
    digit_7.append(train_images[x])
  if train_labels[x] == 8:
    digit_8.append(train_images[x])
  if train_labels[x] == 9:
    digit_9.append(train_images[x])

avg_0 = np.average(digit_0, axis=0)
avg_1 = np.average(digit_1, axis=0)
avg_2 = np.average(digit_2, axis=0)
avg_3 = np.average(digit_3, axis=0)
avg_4 = np.average(digit_4, axis=0)
avg_5 = np.average(digit_5, axis=0)
avg_6 = np.average(digit_6, axis=0)
avg_7 = np.average(digit_7, axis=0)
avg_8 = np.average(digit_8, axis=0)
avg_9 = np.average(digit_9, axis=0)

"""This, while simple and not calculation heavy, resulted in the following 'heat-maps' that clearly demonstrates the 'average' MNIST number for each number 0-9."""

plt.imshow(avg_0)
plt.show()

plt.imshow(avg_1)
plt.show()

plt.imshow(avg_2)
plt.show()

plt.imshow(avg_3)
plt.show()

plt.imshow(avg_4)
plt.show()

plt.imshow(avg_5)
plt.show()

plt.imshow(avg_6)
plt.show()

plt.imshow(avg_7)
plt.show()

plt.imshow(avg_8)
plt.show()

plt.imshow(avg_9)
plt.show()

"""For computing a model, lets look at something thats a bit more calculation heavy.

# **Category 3 - Computing a Model**

Now that we have a basic understanding of what a model is, we can take a look at a more complex one that we used in class.

One of the most important formulas for any statistical analysis is that of linear regression, which leads us to a quick segway.

There are two main types of models - regression models and classification models. Regression models deal with literal values, i.e. it will compute and predict values for a given question, such as what is the percentage chance that it will rain today, or the estimated number of purchased homes in a year. Classification models go the more abstract route, into the realm of discrete math, to answer questions such as if a statement is true or false, or if a given image is an animal, car, or house.

To look at this in more detail, lets go back to the linear regression equation, which is:

    y' = b + w1x1 + w2x2 + ... + wpxp + epsilon(i)

This equation simply means that for a given y' (usually written as y of i, which is impossible to write in colab) we have an intercept at b, then for each feature x we have any number of weights that we can turn into a scalar to multiply by the vector provided by the features x. All this means is that for each weighting of every group of features, we get a value that goes into our equation.

So, lets look at an example (HW1, Question 2) where we plotted random numbers along a graph then performed linear regression on them. The catch was that we had to use the machine to find the equation.
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

import numpy as np
np.random.seed(420)

xs = 2 * np.random.rand(100, 1)
x2 = 2 * np.random.rand(100, 1)
ys = 4 + 3 * xs + np.random.rand(100, 1)

plt.plot(xs, ys, "b.")
plt.plot(x2, ys, "g.")
plt.xlabel("$x_1, x_2$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([0, 2, 0, 15])
plt.show()

"""Now that we have our generated data set, we can start computing our model. The first thing that we're going to do is to split the data into training data and testing data, which covers our next section.

# **Category 4 - Training a Model**

Whenever we train a model, what we're doing is we're telling the machine to generate a set of rules based on the data that it's given. When we use python, we can use the networks provided in numpy to give us a hand with generating these rules for the machine.

It's important to note that we always split or in some way separate our data into two chunks - a training set and a testing set.

The training set is where we actually generate those rules from, and the testing set is where we test those rules. So for now let's continue with our previous example, and split the data.
"""

# training set
train_xs = xs[:80]
train_x2 = x2[:80]
train_ys = ys[:80]
# testing set
test_xs = xs[80:]
test_x2 = x2[80:]
test_ys = ys[80:]

"""With that final piece of pre-computation out of the way, we can get to actually training and computing the model.

We'll use what's called an epoch, which is essentially one 'run-through' of the data set. This means that we examine every single piece of data in the set once and make our predictions or rules from that. Then we run another epoch and compare those results to the previous one. The more epochs we run, the closer we can make our predictions (which falls under fine-tuning our model).

So the first thing we'll do is define the number of epochs that we want to use, as well as how far along our data we want to move for every step of computation.
"""

# number of epochs
epochs = 15
# learning rate
lr = 0.001

"""Next, let's define a starting point for our machine to begin its computation and training on. Since we don't know anything about the data and we don't have any pre-conceived notions about the data, we can just randomly generate it. Here we will also describe our intercept b for the regression equation."""

# initial value for weight w and bias b
w = np.random.randn(1)
w2 = np.random.randn(1) # extension
b = np.zeros(1)

"""Now let's train our model with the epochs that we defined, using our generated data set."""

for epoch in np.arange(epochs):
  for i in np.arange(80):
    y_pred = w * train_xs[i] + w2 * train_x2[i] + b # extension within
    
    grad_w = (y_pred - train_ys[i]) * train_xs[i]
    grad_w2 = (y_pred - train_ys[i]) * train_x2[i] # extension
    grad_b = (y_pred - train_ys[i])
    
    w -= lr * grad_w
    w2 -= lr * grad_w2 # extension
    b -= lr * grad_b

"""For better training results, let's also calculate our test loss."""

test_loss = 0

for i in np.arange(20):
  test_loss += 0.5 * (w * test_xs[i] + w2 * test_x2[i] + b - test_ys[i]) ** 2
test_loss /= 20

"""In the above two snippets, you can see the regression equation that we used for our y_pred and test_loss variables.

Now, we've effectively trained the model and come up with our equation. All that's left to do is make the predictions and plot them.
"""

pred_ys = w * test_xs + w2 * test_x2 + b

plt.plot(test_xs, test_ys, "b.")
plt.plot(test_x2, test_ys, "g.")
plt.plot(test_xs, pred_ys, "r.") # predicted values 
plt.plot(test_x2, pred_ys, "y.")

plt.xlabel("$x_1, x_2$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([0, 2, 0, 15])
plt.show()

"""(Note: slight modifications were made to some numbers to show different results. Original code and results are in the GitHub repository)

# **Category 5 - Finetuning a Pre-Trained Model**

Fine-tuning a model could mean many specific things, but in general it means narrowing down the predictions of a model so that it gets even closer to the desired output as is possible. This could mean making smaller steps along the data set so that we don't miss any data that could affect our prediction, it could mean running and training with a large number of epochs, as in more than 50 epochs or some other large number, it could also mean using an extremely large data set to ensure that all possible cases in the data set are 'experienced' by the machine to help with predictions.

However, going too far with any of these can result in the machine slowing down or otherwise being inefficient when it comes to calculation time of the predictions, and by that point the information could be useless or outdated.

Therefore, fine-tuning a model such as the one above could mean adding more weights to each piece of the regression model, so that we can get even closer to the regression line in the data. This also means using a resonable number of epochs, given the size of the data set and the efficiency and accuracy of a small number of epochs on a data set. Ultimately, its a fine balance between heavy calculation to get closer to the desired output and light calculation to get the rough shape of the data you're currently observing. However, going too light will not provide you with useful data in most cases, and going too heavy will not provide you with useful data in a reasonable timeframe. Fine-tuning then, is that balancing act, tweaking and observing the changes that values make on the system, so that the most efficient and accurate model can be produced when it comes time for predictions.
"""