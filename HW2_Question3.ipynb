{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2_Question3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrJellimann/machine_learning_homework/blob/master/HW2_Question3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGGPXIWEpVNL",
        "colab_type": "text"
      },
      "source": [
        "## **Christopher Walen, COP 4630, Fall 2019**\n",
        "\n",
        "Problem 3\n",
        "\n",
        "In this problem you have to work with the CIFAR10 data set. Check out the notebook cifar10_data_set to see how to load it.\n",
        "\n",
        "Give three convolutional models that\n",
        "\n",
        "first model underfits\n",
        "second model overfits\n",
        "third model is pretty good\n",
        "Make sure that you plot the curves depicting the training/validation accuracy/loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQR-a7723Z1D",
        "colab_type": "text"
      },
      "source": [
        "## **Old Version - Issues and many warnings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZKp9GpZpQAZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82101e59-678d-403e-8ffa-2f270afbfc61"
      },
      "source": [
        "# import keras\n",
        "\n",
        "# from keras.datasets import cifar10\n",
        "\n",
        "# from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "# from keras.layers import AveragePooling2D, Input, Flatten\n",
        "\n",
        "# from keras.optimizers import Adam\n",
        "\n",
        "# from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "# from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# from keras.regularizers import l2\n",
        "\n",
        "# from keras import backend as K\n",
        "\n",
        "# from keras import Model\n",
        "\n",
        "# from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from IPython.display import SVG\n",
        "# %matplotlib inline\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Training Parameters\n",
        "# batch_size = 32\n",
        "# epochs = 10 # default: 200\n",
        "# data_augmentation = True\n",
        "# num_classes = 10\n",
        "\n",
        "# # Subtracting pixel mean improves accuracy\n",
        "# subtract_pixel_mean = True\n",
        "\n",
        "# n = 3\n",
        "# version = 1\n",
        "\n",
        "# if (version == 1):\n",
        "#   depth = n * 6 + 2\n",
        "# elif (version == 2):\n",
        "#   depth = n * 9 + 2\n",
        "\n",
        "# model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# # Load data\n",
        "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# # Normalize\n",
        "# x_train = x_train.astype('float32') / 255\n",
        "# x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# # Convert class vectors to binary class matrices\n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# input_shape = x_train.shape[1:]\n",
        "# num_train = x_train.shape[0]\n",
        "\n",
        "# if (subtract_pixel_mean):\n",
        "#   x_train_mean = np.mean(x_train, axis=0)\n",
        "#   x_train -= x_train_mean\n",
        "#   x_test -= x_train_mean\n",
        "\n",
        "# print('shapes')\n",
        "# print()\n",
        "# print('x_train:', x_train.shape)\n",
        "# print('y_train:', y_train.shape)\n",
        "# print('x_test:', x_test.shape)\n",
        "# print('y_test:', y_test.shape)\n",
        "\n",
        "# def lr_schedule(epoch):\n",
        "#   \"\"\"\n",
        "#   Learning Rate Schedule\n",
        "\n",
        "#   Learning rate is scheduled to be reduced after 100, 150, 200, 225 epochs.\n",
        "#   Called automatically every epoch as part of callbacks during training.\n",
        "  \n",
        "#   # Argument\n",
        "#     epoch (int): The number of epochs\n",
        "    \n",
        "#   # Returns \n",
        "#     lr (float32): learning rate\n",
        "#   \"\"\"\n",
        "\n",
        "#   lr = 1e-3\n",
        "\n",
        "#   if (epoch > 180):\n",
        "#     lr *= 0.5e-3\n",
        "#   elif (epoch > 160):\n",
        "#     lr *= 1e-3\n",
        "#   elif (epoch > 120):\n",
        "#     lr *= 1e-2\n",
        "#   elif (epoch > 80):\n",
        "#     lr *= 1e-1\n",
        "  \n",
        "#   return lr\n",
        "\n",
        "# plt.title('Learning Rate Schedule')\n",
        "# plt.plot(range(epochs), [lr_schedule(epoch) for epoch in range(epochs)])\n",
        "# plt.ylabel('learning rate')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.show()\n",
        "\n",
        "# def resnet_layer(inputs, \n",
        "#                  num_filters=16, \n",
        "#                  kernel_size=3, \n",
        "#                  strides=1,\n",
        "#                  activation='relu', \n",
        "#                  batch_normalization=True, \n",
        "#                  conv_first=True):\n",
        "#   \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "#     # Arguments\n",
        "#         inputs (tensor): input tensor from input image or previous layer\n",
        "#         num_filters (int): Conv2D number of filters\n",
        "#         kernel_size (int): Conv2D square kernel dimensions\n",
        "#         strides (int): Conv2D square stride dimensions\n",
        "#         activation (string): activation name\n",
        "#         batch_normalization (bool): whether to include batch normalization\n",
        "#         conv_first (bool): conv-bn-activation (True) or\n",
        "#             bn-activation-conv (False)\n",
        "\n",
        "#     # Returns\n",
        "#         x (tensor): tensor as input to the next layer\n",
        "#     \"\"\"\n",
        "#   conv = Conv2D(num_filters, \n",
        "#                 kernel_size=kernel_size, \n",
        "#                 strides=strides, \n",
        "#                 padding='same', \n",
        "#                 kernel_initializer='he_normal', \n",
        "#                 kernel_regularizer=l2(1e-4))  \n",
        "  \n",
        "#   x = inputs\n",
        "#   if conv_first:\n",
        "#     # apply conv layer first\n",
        "#     x = conv(x)\n",
        "#     if batch_normalization:\n",
        "#       x = BatchNormalization()(x)\n",
        "#     if activation is not None:\n",
        "#       x = Activation(activation)(x)\n",
        "#   else:\n",
        "#     if batch_normalization:\n",
        "#       x = BatchNormalization()(x)\n",
        "#     if activation is not None:\n",
        "#       x = Activation(activation)(x)\n",
        "#     # apply conv layer last\n",
        "#     x = conv(x)\n",
        "      \n",
        "#   return x  \n",
        "\n",
        "# def resnet_v1(input_shape, depth, num_classes):\n",
        "#   \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "#   Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "#   Last ReLU is after the shortcut connection.\n",
        "#   At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "#   by a convolutional layer with strides=2, while the number of filters is\n",
        "#   doubled. Within each stage, the layers use the same type of filters \n",
        "#   (in terms of size, stride, padding) and the same number of filters.\n",
        "#   Features maps sizes:\n",
        "#   stage 0: 32x32, 16\n",
        "#   stage 1: 16x16, 32\n",
        "#   stage 2:  8x8,  64\n",
        "    \n",
        "#   # Arguments\n",
        "#       input_shape (tensor): shape of input image tensor\n",
        "#       depth (int): number of core convolutional layers\n",
        "#       num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "#   # Returns\n",
        "#       model (Model): Keras model instance\n",
        "#   \"\"\"\n",
        "  \n",
        "#   if (depth - 2) % 6 != 0:\n",
        "#     raise ValueError('depth should be 6n+2 (eg 20, 32, 44)')\n",
        "#   # Start model definition\n",
        "#   num_filters = 1 # default was 16\n",
        "#   num_res_blocks = int((depth - 2) / 6)\n",
        "  \n",
        "#   inputs = Input(shape=input_shape)\n",
        "#   x = resnet_layer(inputs=inputs)\n",
        "  \n",
        "#   # Instantiate the stack of residual inputs\n",
        "#   for stack in range(3):\n",
        "#     for res_block in range(num_res_blocks):\n",
        "      \n",
        "#       strides = 1\n",
        "#       if stack > 0 and res_block == 0: \n",
        "#         # first layer but not first stack\n",
        "#         # downsample\n",
        "#         strides = 2 \n",
        "        \n",
        "#       y = resnet_layer(inputs=x, \n",
        "#                        num_filters=num_filters,\n",
        "#                        strides=strides)\n",
        "#       y = resnet_layer(inputs=y,\n",
        "#                        num_filters=num_filters,\n",
        "#                        activation=None)\n",
        "      \n",
        "#       if stack > 0 and res_block == 0: \n",
        "#         # first layer but not first stack\n",
        "#         # linear projection residual shortcut connection to match\n",
        "#         # changed dims\n",
        "#         x = resnet_layer(inputs=x,\n",
        "#                          num_filters=num_filters,\n",
        "#                          kernel_size=1,\n",
        "#                          strides=strides,\n",
        "#                          activation=None,\n",
        "#                          batch_normalization=False)\n",
        "#       x = keras.layers.add([x,y])\n",
        "#       x = Activation('relu')(x)\n",
        "#     num_filters *=2\n",
        "    \n",
        "#   # Add classifier on top\n",
        "#   # v1 does not use BN after last shortcut connection-ReLU\n",
        "#   x = AveragePooling2D(pool_size=8)(x)\n",
        "#   y = Flatten()(x)\n",
        "#   outputs = Dense(num_classes,\n",
        "#                   activation='softmax',\n",
        "#                   kernel_initializer='he_normal')(y)\n",
        "#   # Instantiate model\n",
        "#   model = Model(inputs=inputs, outputs=outputs)\n",
        "#   return model\n",
        "\n",
        "# model = resnet_v1(input_shape, 20, 10)\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer=Adam(lr=lr_schedule(0)),\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# # Prepare model model saving directory.\n",
        "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "# model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "# if not os.path.isdir(save_dir):\n",
        "#     os.makedirs(save_dir)\n",
        "# filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# # Prepare callbacks for model saving and for learning rate adjustment.\n",
        "# checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "#                              monitor='val_acc',\n",
        "#                              verbose=1,\n",
        "#                              save_best_only=True)\n",
        "\n",
        "# lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "#                                cooldown=0,\n",
        "#                                patience=5,\n",
        "#                                min_lr=0.5e-6)\n",
        "\n",
        "# callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# # Run training, with or without data augmentation.\n",
        "# if not data_augmentation:\n",
        "#     print('Not using data augmentation.')\n",
        "#     history = model.fit(x_train, y_train,\n",
        "#                         batch_size=batch_size,\n",
        "#                         epochs=epochs,\n",
        "#                         validation_data=(x_test, y_test),\n",
        "#                         shuffle=True,\n",
        "#                         verbose=0,\n",
        "#                         callbacks=callbacks)\n",
        "# else:\n",
        "#     print('Using real-time data augmentation.')\n",
        "#     # This will do preprocessing and realtime data augmentation:\n",
        "#     datagen = ImageDataGenerator(\n",
        "#         # set input mean to 0 over the dataset\n",
        "#         featurewise_center=False,\n",
        "#         # set each sample mean to 0\n",
        "#         samplewise_center=False,\n",
        "#         # divide inputs by std of dataset\n",
        "#         featurewise_std_normalization=False,\n",
        "#         # divide each input by its std\n",
        "#         samplewise_std_normalization=False,\n",
        "#         # apply ZCA whitening\n",
        "#         zca_whitening=False,\n",
        "#         # epsilon for ZCA whitening\n",
        "#         zca_epsilon=1e-06,\n",
        "#         # randomly rotate images in the range (deg 0 to 180)\n",
        "#         rotation_range=0,\n",
        "#         # randomly shift images horizontally\n",
        "#         width_shift_range=0.1,\n",
        "#         # randomly shift images vertically\n",
        "#         height_shift_range=0.1,\n",
        "#         # set range for random shear\n",
        "#         shear_range=0.,\n",
        "#         # set range for random zoom\n",
        "#         zoom_range=0.,\n",
        "#         # set range for random channel shifts\n",
        "#         channel_shift_range=0.,\n",
        "#         # set mode for filling points outside the input boundaries\n",
        "#         fill_mode='nearest',\n",
        "#         # value used for fill_mode = \"constant\"\n",
        "#         cval=0.,\n",
        "#         # randomly flip images\n",
        "#         horizontal_flip=True,\n",
        "#         # randomly flip images\n",
        "#         vertical_flip=False,\n",
        "#         # set rescaling factor (applied before any other transformation)\n",
        "#         rescale=None,\n",
        "#         # set function that will be applied on each input\n",
        "#         preprocessing_function=None,\n",
        "#         # image data format, either \"channels_first\" or \"channels_last\"\n",
        "#         data_format=None,\n",
        "#         # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "#         validation_split=0.0)\n",
        "\n",
        "#     # Compute quantities required for featurewise normalization\n",
        "#     # (std, mean, and principal components if ZCA whitening is applied).\n",
        "#     datagen.fit(x_train)\n",
        "\n",
        "#     # Fit the model on the batches generated by datagen.flow().\n",
        "#     history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "#                                   validation_data=(x_test, y_test),\n",
        "#                                   shuffle=True,\n",
        "#                                   epochs=epochs, \n",
        "#                                   steps_per_epoch=num_train // batch_size,\n",
        "#                                   verbose=0, \n",
        "#                                   callbacks=callbacks)\n",
        "    \n",
        "# # Score trained model.\n",
        "# # test\n",
        "# scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "# print('Test loss:', scores[0])\n",
        "# print('Test accuracy:', scores[1])\n",
        "\n",
        "# # Score trained model.\n",
        "# # train\n",
        "# scores = model.evaluate(x_train, y_train, verbose=1)\n",
        "# print('Train loss:', scores[0])\n",
        "# print('Train accuracy:', scores[1])\n",
        "\n",
        "# model.save('cifar10_ResNet20v1_model.h5')\n",
        "\n",
        "# model_fname = 'cifar10_ResNet20v1_model.h5' \n",
        "\n",
        "# from google.colab import files\n",
        "\n",
        "# with open(model_fname, 'r') as f:\n",
        "#   files.download(model_fname)\n",
        "\n",
        "# np.savez('x_train_mean.npz', x_train_mean=x_train_mean)\n",
        "\n",
        "# with open('x_train_mean.npz', 'r') as f:\n",
        "#   files.download('x_train_mean.npz')\n",
        "\n",
        "# acc = history.history['acc']\n",
        "# val_acc = history.history['val_acc']\n",
        "\n",
        "# loss = history.history['loss']\n",
        "# val_loss = history.history['val_loss']\n",
        "\n",
        "# epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# # training and validation accuracy\n",
        "\n",
        "# plt.plot(epochs, acc, 'bo', label='training acc')\n",
        "# plt.plot(epochs, val_acc, 'r', label='test acc')\n",
        "# plt.title('training and test accuracy')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.figure()\n",
        "\n",
        "# # training and validation loss\n",
        "\n",
        "# plt.plot(epochs, loss, 'bo', label='training loss')\n",
        "# plt.plot(epochs, val_loss, 'r', label='test loss')\n",
        "# plt.title('training and test loss')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "# start_epoch = 0 # default was 60\n",
        "\n",
        "# # training and validation accuracy\n",
        "\n",
        "# plt.plot(epochs[start_epoch:], acc[start_epoch:], 'bo', label='training acc')\n",
        "# plt.plot(epochs[start_epoch:], val_acc[start_epoch:], 'r', label='validation acc')\n",
        "# plt.title('training and validation accuracy')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.figure()\n",
        "\n",
        "# # training and validation loss\n",
        "\n",
        "# plt.plot(epochs[start_epoch:], loss[start_epoch:], 'bo', label='training loss')\n",
        "# plt.plot(epochs[start_epoch:], val_loss[start_epoch:], 'r', label='validation loss')\n",
        "# plt.title('training and validation loss')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "shapes\n",
            "\n",
            "x_train: (50000, 32, 32, 3)\n",
            "y_train: (50000, 10)\n",
            "x_test: (10000, 32, 32, 3)\n",
            "y_test: (10000, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Using real-time data augmentation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.20690, saving model to /content/saved_models/cifar10_ResNet20v1_model.001.h5\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.20690 to 0.26170, saving model to /content/saved_models/cifar10_ResNet20v1_model.002.h5\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.26170 to 0.33050, saving model to /content/saved_models/cifar10_ResNet20v1_model.003.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-b3cb569995c6>\", line 322, in <module>\n",
            "    callbacks=callbacks)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1658, in fit_generator\n",
            "    initial_epoch=initial_epoch)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\", line 215, in fit_generator\n",
            "    class_weight=class_weight)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1449, in train_on_batch\n",
            "    outputs = self.train_function(ins)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2979, in __call__\n",
            "    return self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2937, in _call\n",
            "    fetched = self._callable_fn(*array_vals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1472, in __call__\n",
            "    run_metadata_ptr)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
            "    module = self._load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 44, in _load\n",
            "    module = _importlib.import_module(self.__name__)\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/__init__.py\", line 90, in <module>\n",
            "    from tensorflow.contrib import tensor_forest\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/tensor_forest/__init__.py\", line 21, in <module>\n",
            "    from tensorflow.contrib.tensor_forest.client import *\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/tensor_forest/__init__.py\", line 21, in <module>\n",
            "    from tensorflow.contrib.tensor_forest.client import *\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/tensor_forest/client/__init__.py\", line 22, in <module>\n",
            "    from tensorflow.contrib.tensor_forest.client import random_forest\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/tensor_forest/client/random_forest.py\", line 27, in <module>\n",
            "    from tensorflow.contrib.tensor_forest.python import tensor_forest\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/tensor_forest/python/__init__.py\", line 21, in <module>\n",
            "    from tensorflow.contrib.tensor_forest.python import tensor_forest\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/tensor_forest/python/tensor_forest.py\", line 29, in <module>\n",
            "    from tensorflow.contrib.tensor_forest.python.ops import data_ops\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/tensor_forest/python/ops/data_ops.py\", line 20, in <module>\n",
            "    from tensorflow.contrib.tensor_forest.python.ops import tensor_forest_ops\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/tensor_forest/python/ops/tensor_forest_ops.py\", line 28, in <module>\n",
            "    resource_loader.get_path_to_datafile('_tensor_forest_ops.so'))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/util/loader.py\", line 56, in load_op_library\n",
            "    ret = load_library.load_op_library(path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\n",
            "    lib_handle = py_tf.TF_LoadLibrary(library_filename)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rm-VG_S3f0d",
        "colab_type": "text"
      },
      "source": [
        "## **New Version - works better**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVGSANqN0T3l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "cfb2c0da-cd08-44b8-a25b-5c3e85e20e96"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import Model\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from IPython.display import SVG\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 32\n",
        "epochs = 20 # default was 200\n",
        "data_augmentation = False\n",
        "num_classes = 10\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# Model parameters\n",
        "n = 3\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# Load the CIFAR10 data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Input dimensions\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Number of non-test examples\n",
        "num_train = x_train.shape[0] \n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "  x_train_mean = np.mean(x_train, axis=0)\n",
        "  x_train -= x_train_mean\n",
        "  x_test -= x_train_mean\n",
        "\n",
        "print('shapes')\n",
        "print()\n",
        "print('x_train:', x_train.shape)\n",
        "print('y_train:', y_train.shape)\n",
        "print('x_test :', x_test.shape)\n",
        "print('y_test :', y_test.shape)\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \"\"\"Learning Rate Schedule\n",
        "  \n",
        "  Learning rate is scheduled to be reduced after 100, 150, 200, 225 epochs.\n",
        "  Called automatically every epoch as part of callbacks during training.\n",
        "  \n",
        "  # Argument\n",
        "    epoch (int): The number of epochs\n",
        "    \n",
        "  # Returns \n",
        "    lr (float32): learning rate\n",
        "  \"\"\"\n",
        "  \n",
        "  lr = 1e-3\n",
        "  \n",
        "  if epoch > 180:\n",
        "      lr *= 0.5e-3\n",
        "  elif epoch > 160:\n",
        "      lr *= 1e-3\n",
        "  elif epoch > 120:\n",
        "      lr *= 1e-2\n",
        "  elif epoch > 80:\n",
        "      lr *= 1e-1\n",
        "  \n",
        "  return lr\n",
        "  \n",
        "plt.title('Learning rate schedule')\n",
        "plt.plot(range(epochs), [lr_schedule(epoch) for epoch in range(epochs)])\n",
        "plt.ylabel('learning rate')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "\n",
        "def resnet_layer(inputs, \n",
        "                 num_filters=8, # default was 16 \n",
        "                 kernel_size=3, \n",
        "                 strides=1,\n",
        "                 activation='relu', \n",
        "                 batch_normalization=True, \n",
        "                 conv_first=True):\n",
        "  \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "  conv = Conv2D(num_filters, \n",
        "                kernel_size=kernel_size, \n",
        "                strides=strides, \n",
        "                padding='same', \n",
        "                kernel_initializer='he_normal', \n",
        "                kernel_regularizer=l2(1e-4))  \n",
        "  \n",
        "  x = inputs\n",
        "  if conv_first:\n",
        "    # apply conv layer first\n",
        "    x = conv(x)\n",
        "    if batch_normalization:\n",
        "      x = BatchNormalization()(x)\n",
        "    if activation is not None:\n",
        "      x = Activation(activation)(x)\n",
        "  else:\n",
        "    if batch_normalization:\n",
        "      x = BatchNormalization()(x)\n",
        "    if activation is not None:\n",
        "      x = Activation(activation)(x)\n",
        "    # apply conv layer last\n",
        "    x = conv(x)\n",
        "      \n",
        "  return x  \n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes):\n",
        "  \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "  Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "  Last ReLU is after the shortcut connection.\n",
        "  At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "  by a convolutional layer with strides=2, while the number of filters is\n",
        "  doubled. Within each stage, the layers use the same type of filters \n",
        "  (in terms of size, stride, padding) and the same number of filters.\n",
        "  Features maps sizes:\n",
        "  stage 0: 32x32, 16\n",
        "  stage 1: 16x16, 32\n",
        "  stage 2:  8x8,  64\n",
        "    \n",
        "  # Arguments\n",
        "      input_shape (tensor): shape of input image tensor\n",
        "      depth (int): number of core convolutional layers\n",
        "      num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "  # Returns\n",
        "      model (Model): Keras model instance\n",
        "  \"\"\"\n",
        "  \n",
        "  if (depth - 2) % 6 != 0:\n",
        "    raise ValueError('depth should be 6n+2 (eg 20, 32, 44)')\n",
        "  # Start model definition\n",
        "  num_filters = 8 # default was 16\n",
        "  num_res_blocks = int((depth - 2) / 6)\n",
        "  \n",
        "  inputs = Input(shape=input_shape)\n",
        "  x = resnet_layer(inputs=inputs)\n",
        "  \n",
        "  # Instantiate the stack of residual inputs\n",
        "  for stack in range(3):\n",
        "    for res_block in range(num_res_blocks):\n",
        "      \n",
        "      strides = 1\n",
        "      if stack > 0 and res_block == 0: \n",
        "        # first layer but not first stack\n",
        "        # downsample\n",
        "        strides = 2 \n",
        "        \n",
        "      y = resnet_layer(inputs=x, \n",
        "                       num_filters=num_filters,\n",
        "                       strides=strides)\n",
        "      y = resnet_layer(inputs=y,\n",
        "                       num_filters=num_filters,\n",
        "                       activation=None)\n",
        "      \n",
        "      if stack > 0 and res_block == 0: \n",
        "        # first layer but not first stack\n",
        "        # linear projection residual shortcut connection to match\n",
        "        # changed dims\n",
        "        x = resnet_layer(inputs=x,\n",
        "                         num_filters=num_filters,\n",
        "                         kernel_size=1,\n",
        "                         strides=strides,\n",
        "                         activation=None,\n",
        "                         batch_normalization=False)\n",
        "      x = keras.layers.add([x,y])\n",
        "      x = Activation('relu')(x)\n",
        "    num_filters *=2\n",
        "    \n",
        "  # Add classifier on top\n",
        "  # v1 does not use BN after last shortcut connection-ReLU\n",
        "  x = AveragePooling2D(pool_size=8)(x)\n",
        "  y = Flatten()(x)\n",
        "  outputs = Dense(num_classes,\n",
        "                  activation='softmax',\n",
        "                  kernel_initializer='he_normal')(y)\n",
        "  # Instantiate model\n",
        "  model = Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "model = resnet_v1(input_shape, 20, 10)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        shuffle=True,\n",
        "                        verbose=0,\n",
        "                        callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                                  validation_data=(x_test, y_test),\n",
        "                                  shuffle=True,\n",
        "                                  epochs=epochs, \n",
        "                                  steps_per_epoch=num_train // batch_size,\n",
        "                                  verbose=0, \n",
        "                                  callbacks=callbacks)\n",
        "    \n",
        "# Score trained model.\n",
        "# test\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss:', scores[0])\n",
        "print('Test Accuracy:', scores[1])\n",
        "\n",
        "# Score trained model.\n",
        "# train\n",
        "scores = model.evaluate(x_train, y_train, verbose=1)\n",
        "print('Train Loss:', scores[0])\n",
        "print('Train Accuracy:', scores[1])\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# Training and Validation accuracy\n",
        "\n",
        "plt.plot(epochs, acc, 'go', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'm', label='Test Accuracy')\n",
        "plt.title('Training and Test Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Training and Validation loss\n",
        "\n",
        "plt.plot(epochs, loss, 'go', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'm', label='Test Loss')\n",
        "plt.title('Training and Test Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "start_epoch = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shapes\n",
            "\n",
            "x_train: (50000, 32, 32, 3)\n",
            "y_train: (50000, 10)\n",
            "x_test : (10000, 32, 32, 3)\n",
            "y_test : (10000, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Not using data augmentation.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}